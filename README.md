# web_crawler
Web Crawler which, given a URL, will recursively crawl the website and list all of the URLs found within, up to a maximum depth of X.

Bonus points for:
- Support basic HTTP auth
- Allow depth, auth, and URL to be configurable by a flag
- Custom configurable get timeout
- Multiple workers (multithreading)
- Store to disk instead of screen output
- Other features
